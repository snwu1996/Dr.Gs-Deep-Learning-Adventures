{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, ligids, smiles, scores, autoreset=False):\n",
    "        self.ligids = ligids\n",
    "        self.smiles = smiles\n",
    "        self.scores = scores\n",
    "        self.autoreset = autoreset\n",
    "        self.num_ligids = int(ligids.shape[0])\n",
    "        self.num_smiles = int(smiles.shape[0])\n",
    "        self.num_scores = int(scores.shape[0])\n",
    "        self.batch_index = 0 # Current batch index\n",
    "    \n",
    "        assert(self.num_ligids*self.num_smiles == self.num_scores),\\\n",
    "        'number of ligids times number of smiles must equal number of scores'\n",
    "    \n",
    "    def next_batch(self, batch_size):                \n",
    "        assert(self.batch_index < self.num_scores), \\\n",
    "        'batch index out of bound, try doing Data.reset() after stepping through the entire dataset'\n",
    "        \n",
    "        lig_idx_lower = int(self.batch_index/self.num_smiles)\n",
    "        lig_idx_upper = int((self.batch_index+batch_size-1)/self.num_smiles)\n",
    "        smi_idx_lower = self.batch_index-lig_idx_lower*self.num_smiles\n",
    "        smi_idx_upper = smi_idx_lower+batch_size\n",
    "        smi_idx_upper -= int((smi_idx_upper-1)/self.num_smiles)*self.num_smiles\n",
    "        \n",
    "        if lig_idx_upper-lig_idx_lower == 0:\n",
    "            ligids_batch = self.ligids[lig_idx_lower,:]\n",
    "            ligids_batch = np.tile(ligids_batch, (batch_size,1))\n",
    "            smiles_batch = self.smiles[smi_idx_lower:smi_idx_upper,:]\n",
    "            \n",
    "        if lig_idx_upper-lig_idx_lower == 1:\n",
    "            ligids_batch1 = self.ligids[lig_idx_lower,:]\n",
    "            ligids_batch1 = np.tile(ligids_batch1, (self.num_smiles-smi_idx_lower,1))\n",
    "            ligids_batch2 = self.ligids[lig_idx_upper,:]\n",
    "            ligids_batch2 = np.tile(ligids_batch2, (smi_idx_upper,1))\n",
    "            ligids_batch = np.concatenate((ligids_batch1,ligids_batch2), axis=0)\n",
    "\n",
    "            smiles_batch1 = self.smiles[smi_idx_lower:,:]\n",
    "            smiles_batch2 = self.smiles[:smi_idx_upper,:]\n",
    "            smiles_batch = np.concatenate((smiles_batch1,smiles_batch2), axis=0)\n",
    "           \n",
    "        if lig_idx_upper-lig_idx_lower >= 2:\n",
    "            raise Exception('batch size too large')\n",
    "           \n",
    "        scores_batch = self.scores[self.batch_index:self.batch_index+batch_size]\n",
    "        self.batch_index += batch_size\n",
    "        return ligids_batch, smiles_batch, scores_batch\n",
    "\n",
    "    def full_batch(self):\n",
    "        raise NotImplementedError('full_batch not implemented')\n",
    "    \n",
    "    def random_batch(self, batch_size):\n",
    "        raise NotImplementedError('random_batch not implemented')\n",
    "    \n",
    "    def shuffle(self):\n",
    "        raise NotImplementedError('shuffle not implemented')\n",
    "    \n",
    "    def reset(self,shuffle=False):\n",
    "        self.batch_index = 0\n",
    "        if shuffle:\n",
    "            self.shuffle()\n",
    "            \n",
    "########################################################################################\n",
    "\n",
    "def train_validation_split(ligids, smiles, labels, num_val_lig=3046, num_val_smi=10581):\n",
    "    \"\"\"\n",
    "    Example usage:\n",
    "        train_data, validation_data = train_validation_split(train_valid_ligids,\n",
    "                                                             train_valid_smiles,\n",
    "                                                             train_valid_scores,\n",
    "                                                             num_val_lig=3046, \n",
    "                                                             num_val_smi=10581)\n",
    "    \"\"\"\n",
    "    # Train valiatation split - X data\n",
    "    num_train_lig = ligids.shape[0]-num_val_lig\n",
    "    num_train_smi = smiles.shape[0]-num_val_smi\n",
    "\n",
    "    print('num validation ligids: {}'.format(num_val_lig))\n",
    "    print('num train ligids: {}'.format(num_train_lig))\n",
    "    print('num validation smiles: {}'.format(num_val_smi))\n",
    "    print('num train smiles: {}'.format(num_train_smi))\n",
    "\n",
    "    train_ligids = ligids[:num_train_lig,:]\n",
    "    train_smiles = smiles[:num_train_smi,:]\n",
    "    validation_ligids = ligids[num_train_lig:,:]\n",
    "    validation_smiles = smiles[num_train_smi:,:]\n",
    "\n",
    "    # Train validation split - Y data\n",
    "    train_labels = []\n",
    "    validation_labels = []\n",
    "    data = Data(ligids, smiles, labels)\n",
    "    for lig_num in range(num_train_lig): # Train labels\n",
    "        _, _, train_labels_batch = data.next_batch(num_train_smi)\n",
    "        _, _, _ = data.next_batch(num_val_smi)\n",
    "        train_labels.append(train_labels_batch)\n",
    "    for lig_num in range(num_val_lig): # Validation labels\n",
    "        _, _, _ = data.next_batch(num_train_smi)\n",
    "        _, _, validation_labels_batch = data.next_batch(num_val_smi)\n",
    "        validation_labels.append(validation_labels_batch)\n",
    "    train_labels = np.concatenate(train_labels, axis=0)\n",
    "    validation_labels = np.concatenate(validation_labels, axis=0)\n",
    "    print('num validation labels: {}'.format(validation_labels.shape[0]))\n",
    "    print('num train labels: {}'.format(train_labels.shape[0]))\n",
    "\n",
    "    # Return train and validation datasets\n",
    "    train_data = Data(train_ligids, train_smiles, train_labels)\n",
    "    validation_data = Data(validation_ligids, validation_smiles, validation_labels)\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_ligids  = np.load('../data/PHARM_TRAIN_X.npy')\n",
    "train_valid_smiles = np.load('../data/PHARM_TRAIN_SMILES.npy')\n",
    "train_valid_scores = np.load('../data/Y_train.npy')\n",
    "test_ligids = np.load('../data/PHARM_TEST_X.npy')\n",
    "test_smiles = np.load('../data/PHARM_TEST_SMILES.npy')\n",
    "\n",
    "print('train_valid_ligids shape: {}'.format(train_valid_ligids.shape))\n",
    "print('train_valid_smiles shape: {}'.format(train_valid_smiles.shape))\n",
    "print('train_valid_scores shape: {}'.format(train_valid_scores.shape))\n",
    "print('test_ligids shape: {}'.format(test_ligids.shape))\n",
    "print('test_smiles shape: {}'.format(test_smiles.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = train_validation_split(train_valid_ligids,\n",
    "                                                     train_valid_smiles,\n",
    "                                                     train_valid_scores,\n",
    "                                                     num_val_lig=3046, \n",
    "                                                     num_val_smi=10581)\n",
    "test_data = Data(test_ligids, \n",
    "                 test_smiles, \n",
    "                 np.empty(shape=(test_ligids.shape[0]* test_smiles.shape[0]), dtype=np.int8))\n",
    "\n",
    "del train_valid_ligids, train_valid_smiles, train_valid_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = .001\n",
    "LAMBDA = .001\n",
    "DROPOUT = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1_UNITS = 100\n",
    "L2_UNITS = 100\n",
    "L3_UNITS = 100\n",
    "NUM_OUTPUTS = 1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "relu = tf.nn.relu\n",
    "xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "zero_init = tf.zeros_initializer()\n",
    "l2_reg = tf.contrib.layers.l2_regularizer(scale=LAMBDA)\n",
    "\n",
    "\n",
    "with tf.name_scope('inputs') as scope:\n",
    "    X = tf.placeholder(shape=(None, 176), dtype=tf.float32, name='ligids_smiles')\n",
    "    Y = tf.placeholder(shape=(None), dtype=tf.float32, name='score')\n",
    "    training = tf.placeholder_with_default(input=False, shape=(), name='training')\n",
    "    \n",
    "with tf.name_scope('hidden_layers') as scope:\n",
    "    layer1 = tf.layers.dense(inputs=X,\n",
    "                             units=L1_UNITS, \n",
    "                             activation=relu,\n",
    "                             kernel_initializer=xavier_init,\n",
    "                             bias_initializer=zero_init,\n",
    "                             kernel_regularizer=l2_reg,\n",
    "                             bias_regularizer=l2_reg,\n",
    "                             name='layer1')\n",
    "    layer2 = tf.layers.dense(inputs=layer1, \n",
    "                             units=L2_UNITS, \n",
    "                             activation=relu,\n",
    "                             kernel_initializer=xavier_init,\n",
    "                             bias_initializer=zero_init,\n",
    "                             kernel_regularizer=l2_reg,\n",
    "                             bias_regularizer=l2_reg,\n",
    "                             name='layer2')\n",
    "    layer3 = tf.layers.dense(inputs=layer2, \n",
    "                             units=L3_UNITS, \n",
    "                             activation=relu,\n",
    "                             kernel_initializer=xavier_init,\n",
    "                             bias_initializer=zero_init,\n",
    "                             kernel_regularizer=l2_reg,\n",
    "                             bias_regularizer=l2_reg,\n",
    "                             name='layer3')\n",
    "    \n",
    "with tf.name_scope('predicted_score') as scope:\n",
    "    pred_score = tf.layers.dense(inputs=layer3,\n",
    "                                 units=NUM_OUTPUTS)\n",
    "    with tf.get_default_graph().gradient_override_map({\"Floor\": \"Identity\"}):\n",
    "        pred_score = tf.floor(pred_score)\n",
    "        \n",
    "with tf.name_scope('train') as scope:\n",
    "    reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    train_loss = tf.losses.mean_squared_error(labels=Y,predictions=pred_score)\n",
    "    loss = train_loss+tf.reduce_sum(reg_loss)\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. constants\n",
    "NUM_EPOCHS = 5\n",
    "NUM_SAVES_PER_EPOCH = 5\n",
    "\n",
    "# Saver\n",
    "tf.get_collection('pred_ops')\n",
    "tf.add_to_collection('pred_ops', X)\n",
    "tf.add_to_collection('pred_ops', Y)\n",
    "tf.add_to_collection('pred_ops', pred_score)\n",
    "tf.get_collection('train_ops')\n",
    "tf.add_to_collection('train_ops', X)\n",
    "tf.add_to_collection('train_ops', Y)\n",
    "tf.add_to_collection('train_ops', loss)\n",
    "tf.add_to_collection('train_ops', train_op)\n",
    "saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "# Batches\n",
    "TRAINING_BATCH_SIZE = 10000\n",
    "num_training_batches = int(train_data.num_scores/TRAINING_BATCH_SIZE)\n",
    "VALIDATION_BATCH_SIZE = 10000\n",
    "num_validation_batches = int(validation_data.num_scores/VALIDATION_BATCH_SIZE)\n",
    "\n",
    "# Tensorboard\n",
    "tensorboard_logdir = '../tf_log/run-1'\n",
    "print('tensorboard log_dir: {}\\n'.format(tensorboard_logdir))\n",
    "writer = tf.summary.FileWriter(tensorboard_logdir)\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "# Start Session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print('Beginning epoch {}'.format(epoch))\n",
    "        \n",
    "        # Train Model\n",
    "        train_data.reset()\n",
    "        for i in range(num_training_batches):\n",
    "            ligids_batch, smiles_batch, scores_batch = train_data.next_batch(TRAINING_BATCH_SIZE)\n",
    "            lig_smi_batch = np.concatenate((ligids_batch,smiles_batch), axis=1)\n",
    "            _, train_loss = sess.run([train_op, loss], feed_dict={X:lig_smi_batch, Y:scores_batch})\n",
    "            print('{}/{} train_loss: {}'.format(i, num_training_batches, train_loss), end='\\r')\n",
    "        \n",
    "        # Validation\n",
    "        validation_data.reset()\n",
    "        validation_loss = []\n",
    "        mean_abs_err = []\n",
    "        for i in range(num_validation_batches): \n",
    "            # Compute validation loss batch by batch and average\n",
    "            ligids_batch, smiles_batch, scores_batch = validation_data.next_batch(VALIDATION_BATCH_SIZE)\n",
    "            lig_smi_batch = np.concatenate((ligids_batch,smiles_batch), axis=1)\n",
    "            validation_loss_batch, mean_abs_err_batch = sess.run([loss, mean_abs_err], \n",
    "                                                                 feed_dict={X:lig_smi_batch, Y:scores_batch})\n",
    "            validation_loss.append(validation_loss_batch)\n",
    "            print('{}/{} validation_loss_batch: {}'.format(i, num_validation_batches, validation_loss_batch), end='\\r')\n",
    "        validation_loss = sum(validation_loss)/len(validation_loss)\n",
    "        print('EPOCH: {} | validation loss: {} | mae: {}'.format(epoch, validation_loss, mae))\n",
    "\n",
    "        # Save Model w/ name: e{epoch number}_l{loss}\n",
    "        saver_filename = 'e{}_l{}'.format(epoch, validation_loss)        \n",
    "        saver.save(sess, '../models/{}'.format(saver_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches\n",
    "TESTING_BATCH_SIZE = 10000\n",
    "num_test_batches = int(test_data.num_scores/TESTING_BATCH_SIZE)\n",
    "\n",
    "# Loader\n",
    "saver_filename = 'e8_l2.8511100673734733'\n",
    "tf.reset_default_graph()\n",
    "loader = tf.train.import_meta_graph('../models/{}.meta'.format(saver_filename))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load model and tensors\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loader.restore(sess, 'models/{}'.format(saver_filename))\n",
    "    X, Y, pred_score = tf.get_collection('pred_ops')\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = []\n",
    "    test_data.reset()\n",
    "    for batch_num in range(num_test_batches):\n",
    "        ligids_batch, smiles_batch, empty_batch = test_data.next_batch(TESTING_BATCH_SIZE)\n",
    "        lig_smi_batch = np.concatenate((ligids_batch,smiles_batch), axis=1)\n",
    "        prediction_batch = sess.run(pred_score, feed_dict={X:lig_smi_batch, Y:empty_batch})\n",
    "        predictions.append(prediction_batch)\n",
    "        print('{}/{}'.format(batch_num, num_test_batches), end='\\r')\n",
    "    ligids_batch, smiles_batch, empty_batch = \\\n",
    "    test_data.next_batch(test_data.num_scores-num_test_batches*TESTING_BATCH_SIZE)\n",
    "    lig_smi_batch = np.concatenate((ligids_batch,smiles_batch), axis=1)\n",
    "    prediction_batch = sess.run(pred_score, feed_dict={X:lig_smi_batch, Y:empty_batch})\n",
    "    predictions.append(prediction_batch)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Submission file\n",
    "with open('../submissions/submission_{}.csv'.format(saver_filename), 'w') as out_file\n",
    "    out_file.write('ligid_pharmid,score\\n')\n",
    "    for glob_index in range(test_data.num_scores):\n",
    "        out_file.write('{},{}\\n'.format(glob_index, int(predictions[glob_index])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
